{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] MSFU-Block:------------ \n",
      "------ in_filters:  96\n",
      "------ out_filters:  3\n",
      "[INFO] MSFU-Block:------------ \n",
      "------ in_filters:  192\n",
      "------ out_filters:  96\n",
      "[INFO] MSFU-Block:------------ \n",
      "------ in_filters:  384\n",
      "------ out_filters:  192\n",
      "[INFO] MSFU-Block:------------ \n",
      "------ in_filters:  768\n",
      "------ out_filters:  384\n",
      "[CHECK] x_1.shape torch.Size([12, 1, 256, 256])\n",
      "[DONE] DS blocks\n",
      "[DONE] BTM blocks\n",
      "[DONE] MMFF blocks\n",
      "len_ds_list =  4\n",
      "x_mmff_5.shape =  torch.Size([12, 768, 116, 116])\n",
      "x_mmff_4.shape =  torch.Size([12, 384, 240, 240])\n",
      "x_mmff_3.shape =  torch.Size([12, 192, 244, 244])\n",
      "x_mmff_2.shape =  torch.Size([12, 96, 248, 248])\n",
      "x_mmff_1.shape =  torch.Size([12, 3, 252, 252])\n",
      "[SHAPE] x_low before:  torch.Size([12, 768, 116, 116])\n",
      "[COUNTER]  counter  =  1\n",
      "[SHAPE] x_low after:  torch.Size([12, 384, 240, 240])\n",
      "[SHAPE] x_high:  torch.Size([12, 384, 240, 240])\n",
      "[SHAPE 2222222222222] x_msfu_4.shape =  torch.Size([12, 384, 238, 238])\n",
      "[i] =  2\n",
      "[SHAPE] x_low before:  torch.Size([12, 192, 244, 244])\n",
      "[COUNTER]  counter  =  2\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "running_mean should contain 192 elements not 384",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 196\u001b[0m\n\u001b[1;32m    194\u001b[0m x_3 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(\u001b[39m12\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m256\u001b[39m, \u001b[39m256\u001b[39m)\n\u001b[1;32m    195\u001b[0m multi_branch \u001b[39m=\u001b[39m MultiBranch2DNet()\n\u001b[0;32m--> 196\u001b[0m multi_branch(x_1, x_2, x_3)\n",
      "File \u001b[0;32m~/anaconda3/envs/tiramisu/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[3], line 186\u001b[0m, in \u001b[0;36mMultiBranch2DNet.forward\u001b[0;34m(self, x_1, x_2, x_3)\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mreversed\u001b[39m(\u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlen_ds_list \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)):\n\u001b[1;32m    185\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m[i] = \u001b[39m\u001b[39m'\u001b[39m, i)\n\u001b[0;32m--> 186\u001b[0m     \u001b[39msetattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mx_msfu_\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(i\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mMSFU_list[i](\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mx_mmff_\u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39m+\u001b[39;49m \u001b[39mstr\u001b[39;49m(i\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m)), \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mx_msfu_\u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39m+\u001b[39;49m \u001b[39mstr\u001b[39;49m(i\u001b[39m+\u001b[39;49m\u001b[39m2\u001b[39;49m))))\n\u001b[1;32m    187\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m[DONE] MSFU blocks\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    189\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mx_msfu_1\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/tiramisu/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[3], line 114\u001b[0m, in \u001b[0;36mMSFU_Block.forward\u001b[0;34m(self, x_low, x_high)\u001b[0m\n\u001b[1;32m    112\u001b[0m counter \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    113\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m[COUNTER]  counter  = \u001b[39m\u001b[39m'\u001b[39m, counter)\n\u001b[0;32m--> 114\u001b[0m x_low \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbn1(x_low)\n\u001b[1;32m    115\u001b[0m x_low \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(x_low)\n\u001b[1;32m    116\u001b[0m x_low \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupsample(x_low)\n",
      "File \u001b[0;32m~/anaconda3/envs/tiramisu/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/tiramisu/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    164\u001b[0m     bn_training \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_mean \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_var \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    166\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[39mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[39mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[39mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m    172\u001b[0m     \u001b[39minput\u001b[39;49m,\n\u001b[1;32m    173\u001b[0m     \u001b[39m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_mean\n\u001b[1;32m    175\u001b[0m     \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats\n\u001b[1;32m    176\u001b[0m     \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    177\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_var \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    178\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m    179\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m    180\u001b[0m     bn_training,\n\u001b[1;32m    181\u001b[0m     exponential_average_factor,\n\u001b[1;32m    182\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps,\n\u001b[1;32m    183\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/tiramisu/lib/python3.8/site-packages/torch/nn/functional.py:2450\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2447\u001b[0m \u001b[39mif\u001b[39;00m training:\n\u001b[1;32m   2448\u001b[0m     _verify_batch_size(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[0;32m-> 2450\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m   2451\u001b[0m     \u001b[39minput\u001b[39;49m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled\n\u001b[1;32m   2452\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: running_mean should contain 192 elements not 384"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "class DS_Block(nn.Module):\n",
    "    def __init__(self, in_filters, out_filters, kernel_size = 3):\n",
    "        super().__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_filters)\n",
    "        self.conv1 = nn.Conv2d(in_filters, out_filters, kernel_size)\n",
    "        self.bn2 = nn.BatchNorm2d(out_filters)\n",
    "        self.conv2 = nn.Conv2d(out_filters, out_filters, kernel_size)\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        x = self.bn1(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.conv2(x)\n",
    "        x_pool = self.maxpool(x)\n",
    "        x_skip = x\n",
    "        return x_skip, x_pool\n",
    "\n",
    "class Bottom_Block(nn.Module):\n",
    "    def __init__(self, in_filters, out_filters, kernel_size = 3):\n",
    "        super().__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_filters)\n",
    "        self.conv1 = nn.Conv2d(in_filters, out_filters, kernel_size)\n",
    "        self.bn2 = nn.BatchNorm2d(out_filters)\n",
    "        self.conv2 = nn.Conv2d(out_filters, out_filters, kernel_size)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        x = self.bn1(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "    \n",
    "class MMFF_Block(nn.Module):\n",
    "    '''Multi-Modality Feature Fusion block'''\n",
    "    def __init__(self, in_filters, out_filters):\n",
    "        super().__init__()\n",
    "       \n",
    "        self.bn1_1 = nn.BatchNorm2d(in_filters)        \n",
    "        self.bn1_2 = nn.BatchNorm2d(in_filters)\n",
    "        self.bn1_3 = nn.BatchNorm2d(in_filters)\n",
    "        # 1x1 conv\n",
    "        self.conv1_1 = nn.Conv2d(in_filters, out_filters, 1)\n",
    "        self.conv1_2 = nn.Conv2d(in_filters, out_filters, 1)\n",
    "        self.conv1_3 = nn.Conv2d(in_filters, out_filters, 1)\n",
    "        \n",
    "        # batch norm\n",
    "        self.bn2_1 = nn.BatchNorm2d(out_filters)\n",
    "        self.bn2_2 = nn.BatchNorm2d(out_filters)\n",
    "        self.bn2_3 = nn.BatchNorm2d(out_filters)\n",
    "        \n",
    "        # 3x3 conv\n",
    "        self.conv2_1 = nn.Conv2d(out_filters, out_filters, 3, padding = 1)\n",
    "        self.conv2_2 = nn.Conv2d(out_filters, out_filters, 3, padding = 1)\n",
    "        self.conv2_3 = nn.Conv2d(out_filters, out_filters, 3, padding = 1)\n",
    "        \n",
    "    def forward(self, x_skip_1, x_skip_2, x_skip_3):\n",
    "        \n",
    "        x_1 = self.bn1_1(x_skip_1)\n",
    "        x_1 = self.conv1_1(x_1)\n",
    "        x_1 = self.bn2_1(x_1)\n",
    "        x_1 = self.conv2_1(x_1)\n",
    "        \n",
    "        x_2 = self.bn1_2(x_skip_2)\n",
    "        x_2 = self.conv1_2(x_2)\n",
    "        x_2 = self.bn2_2(x_2)\n",
    "        x_2 = self.conv2_2(x_2)\n",
    "        \n",
    "        x_3 = self.bn1_3(x_skip_3)\n",
    "        x_3 = self.conv1_3(x_3)\n",
    "        x_3 = self.bn2_3(x_3)\n",
    "        x_3 = self.conv2_3(x_3)\n",
    "        \n",
    "        # concat all x_1, x_2, x_3\n",
    "        x = torch.cat((x_1, x_2, x_3), dim = 1)\n",
    "        return x\n",
    "\n",
    "counter = 0 \n",
    "class MSFU_Block(nn.Module):\n",
    "    '''\n",
    "    Multi-Scale Feature Up-sampling block\n",
    "    '''\n",
    "    def __init__(self, in_filters, out_filters):\n",
    "        super().__init__()\n",
    "        in_filters = in_filters * 3\n",
    "        out_filters = out_filters * 3\n",
    "        print('[INFO] MSFU-Block:------------ ')\n",
    "        print('------ in_filters: ', in_filters)\n",
    "        print('------ out_filters: ', out_filters)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(in_filters)\n",
    "        self.conv1 = nn.Conv2d(in_filters, out_filters, kernel_size = 1, padding = 2)\n",
    "        # 2x2 upsample\n",
    "        self.upsample = nn.Upsample(scale_factor = 2, mode = 'bilinear', align_corners = True)\n",
    "        \n",
    "        # CONCATENATE\n",
    "        # after concate\n",
    "        self.bn2 = nn.BatchNorm2d(out_filters*2)        \n",
    "        self.conv2 = nn.Conv2d(out_filters*2, out_filters, 1)\n",
    "        self.bn3 = nn.BatchNorm2d(out_filters)\n",
    "        self.conv3 = nn.Conv2d(out_filters, out_filters, 3)\n",
    "\n",
    "    def forward(self, x_low, x_high):\n",
    "        ''' \n",
    "        low-resolution input to the MSFU-Block came from the bottom- or another MSFU-Block\n",
    "        high-resolution came from MMFF-Block\n",
    "        '''\n",
    "        print('[SHAPE] x_low before: ', x_low.shape)\n",
    "        global counter\n",
    "        counter += 1\n",
    "        print('[COUNTER]  counter  = ', counter)\n",
    "        x_low = self.bn1(x_low)\n",
    "        x_low = self.conv1(x_low)\n",
    "        x_low = self.upsample(x_low)\n",
    "        \n",
    "        print('[SHAPE] x_low after: ', x_low.shape)\n",
    "        print('[SHAPE] x_high: ', x_high.shape)\n",
    "        x = torch.cat((x_low, x_high), dim = 1)\n",
    "        x = self.bn2(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.conv3(x)\n",
    "        return x\n",
    "\n",
    "class MultiBranch2DNet(nn.Module):\n",
    "    def __init__(self, filter_list_in = [1, 32, 64, 128, 256, 512], filter_list_out = [1, 32, 64, 128, 256, 512]):\n",
    "        # filter_list_in = [3, 32, 64, 128, 256, 512]\n",
    "        # filter_list_out = [1, 32, 64, 128, 256, 512]\n",
    "        # len(filter_list_in) == len(filter_list_out)\n",
    "        super().__init__()\n",
    "        self.DS_list_1 = [DS_Block(filter_list_in[i], filter_list_in[i+1]) for i in range(len(filter_list_in)-2)]\n",
    "        self.DS_list_2 = [DS_Block(filter_list_in[i], filter_list_in[i+1]) for i in range(len(filter_list_in)-2)]\n",
    "        self.DS_list_3 = [DS_Block(filter_list_in[i], filter_list_in[i+1]) for i in range(len(filter_list_in)-2)]\n",
    "        \n",
    "        self.BTM_1 = Bottom_Block(filter_list_in[-2], filter_list_in[-1])\n",
    "        self.BTM_2 = Bottom_Block(filter_list_in[-2], filter_list_in[-1])\n",
    "        self.BTM_3 = Bottom_Block(filter_list_in[-2], filter_list_in[-1])\n",
    "        \n",
    "        self.MMFF_list = [MMFF_Block(filter_list_out[i+1], filter_list_out[i]) for i in range(len(filter_list_out)-1)]\n",
    "        self.MSFU_list = [MSFU_Block(filter_list_out[i+1], filter_list_out[i]) for i in range(len(filter_list_out)-2)]\n",
    "        self.len_ds_list = len(self.DS_list_1 )\n",
    "    def forward(self, x_1, x_2, x_3):\n",
    "        # use setattr to create new variables\n",
    "        # init input\n",
    "        print('[CHECK] x_1.shape', x_1.shape)\n",
    "        setattr(self, 'x_ds_1_0_skip', x_1)\n",
    "        setattr(self, 'x_ds_2_0_skip', x_2)\n",
    "        setattr(self, 'x_ds_3_0_skip', x_3)\n",
    "        \n",
    "        # pass through DS blocks                \n",
    "        for i in [1, 2, 3]:\n",
    "            for j in range(self.len_ds_list):\n",
    "                output_ds = self.DS_list_1[j](getattr(self, f'x_ds_{i}_{j}_skip'))\n",
    "                setattr(self, f'x_ds_{i}_{j+1}_skip', output_ds[0])\n",
    "                setattr(self, f'x_ds_{i}_{j+1}_pool', output_ds[1])\n",
    "        print('[DONE] DS blocks') \n",
    "\n",
    "        x_btm_1 = self.BTM_1(getattr(self, 'x_ds_1_'+str(self.len_ds_list) + '_pool'))\n",
    "        x_btm_2 = self.BTM_2(getattr(self, 'x_ds_2_'+str(self.len_ds_list) + '_pool'))\n",
    "        x_btm_3 = self.BTM_3(getattr(self, 'x_ds_3_'+str(self.len_ds_list) + '_pool'))\n",
    "        print('[DONE] BTM blocks')        \n",
    "        \n",
    "        setattr(self, 'x_mmff_5', self.MMFF_list[self.len_ds_list](x_btm_1, x_btm_2, x_btm_3))\n",
    "        for i in reversed(range(self.len_ds_list)):\n",
    "            setattr(self, 'x_mmff_' + str(i+1), self.MMFF_list[i](\n",
    "                                    getattr(self, 'x_ds_1_'+str(i+1) + '_skip'), \\\n",
    "                                    getattr(self, 'x_ds_2_'+str(i+1) + '_skip'), \\\n",
    "                                    getattr(self, 'x_ds_3_'+str(i+1) + '_skip')))\n",
    "        print('[DONE] MMFF blocks')\n",
    "        \n",
    "        print('len_ds_list = ', self.len_ds_list)\n",
    "        print('x_mmff_5.shape = ', getattr(self, 'x_mmff_5').shape)\n",
    "        print('x_mmff_4.shape = ', getattr(self, 'x_mmff_4').shape)\n",
    "        print('x_mmff_3.shape = ', getattr(self, 'x_mmff_3').shape)\n",
    "        print('x_mmff_2.shape = ', getattr(self, 'x_mmff_2').shape)\n",
    "        print('x_mmff_1.shape = ', getattr(self, 'x_mmff_1').shape)\n",
    "        # print('x_mmff_0.shape = ', getattr(self, 'x_mmff_0').shape)\n",
    "        \n",
    "        setattr(self, 'x_msfu_4' ,  self.MSFU_list[self.len_ds_list - 1](getattr(self, 'x_mmff_5'), getattr(self, 'x_mmff_4')))\n",
    "        print('[SHAPE 2222222222222] x_msfu_4.shape = ', getattr(self, 'x_msfu_4').shape)\n",
    "        \n",
    "        for i in reversed(range(self.len_ds_list - 1)):\n",
    "            print('[i] = ', i)\n",
    "            setattr(self, 'x_msfu_' + str(i+1), self.MSFU_list[i](getattr(self, 'x_mmff_' + str(i+1)), getattr(self, 'x_msfu_' + str(i+2))))\n",
    "        print('[DONE] MSFU blocks')\n",
    "        \n",
    "        x = getattr(self, 'x_msfu_1')\n",
    "        return x \n",
    "    \n",
    "x_1 = torch.randn(12, 1, 256, 256)\n",
    "x_2 = torch.randn(12, 1, 256, 256)\n",
    "x_3 = torch.randn(12, 1, 256, 256)\n",
    "multi_branch = MultiBranch2DNet()\n",
    "multi_branch(x_1, x_2, x_3)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "x_msfu_4 = torch.rand([12, 384, 238, 238])\n",
    "x = torch.rand([12, 384, 240, 240])\n",
    "y = torch.rand([12, 384, 240, 240])\n",
    "# x_mmff_5.shape =  torch.Size([12, 768, 116, 116])\n",
    "# x_mmff_4.shape =  torch.Size([12, 384, 240, 240])\n",
    "# x_mmff_3.shape =  torch.Size([12, 192, 244, 244])\n",
    "# x_mmff_2.shape =  torch.Size([12, 96, 248, 248])\n",
    "# x_mmff_1.shape =  torch.Size([12, 3, 252, 252])\n",
    "msfu_ok   = MSFU_Block(256, 128)\n",
    "msfu_ok(x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-8.2347e-03, -4.9215e-02, -9.0194e-02,  ..., -4.4169e-01,\n",
       "           -3.1098e-01, -1.8027e-01],\n",
       "          [-6.5949e-02, -1.1866e-01, -1.7137e-01,  ..., -5.0108e-01,\n",
       "           -5.3456e-01, -5.6805e-01],\n",
       "          [-1.2366e-01, -1.8811e-01, -2.5255e-01,  ..., -5.6046e-01,\n",
       "           -7.5815e-01, -9.5583e-01],\n",
       "          ...,\n",
       "          [-2.5923e-01, -6.6383e-01, -1.0684e+00,  ..., -3.2061e-01,\n",
       "           -3.3419e-01, -3.4778e-01],\n",
       "          [-1.4079e-02, -4.6083e-01, -9.0759e-01,  ..., -4.4952e-01,\n",
       "           -4.3964e-01, -4.2975e-01],\n",
       "          [ 2.3108e-01, -2.5783e-01, -7.4674e-01,  ..., -5.7843e-01,\n",
       "           -5.4508e-01, -5.1173e-01]],\n",
       "\n",
       "         [[-3.0458e-02, -3.3570e-02, -3.6681e-02,  ..., -6.1116e-01,\n",
       "           -3.8773e-01, -1.6430e-01],\n",
       "          [ 1.6179e-01,  8.9186e-02,  1.6585e-02,  ..., -4.0656e-01,\n",
       "           -2.3250e-01, -5.8451e-02],\n",
       "          [ 3.5403e-01,  2.1194e-01,  6.9850e-02,  ..., -2.0195e-01,\n",
       "           -7.7276e-02,  4.7403e-02],\n",
       "          ...,\n",
       "          [ 1.1600e-01,  8.4787e-02,  5.3570e-02,  ...,  3.8821e-02,\n",
       "            4.9231e-02,  5.9642e-02],\n",
       "          [ 1.6059e-01,  9.7950e-02,  3.5307e-02,  ...,  9.4305e-02,\n",
       "           -5.0569e-02, -1.9544e-01],\n",
       "          [ 2.0518e-01,  1.1111e-01,  1.7043e-02,  ...,  1.4979e-01,\n",
       "           -1.5037e-01, -4.5053e-01]],\n",
       "\n",
       "         [[-3.8419e-01, -3.2355e-01, -2.6291e-01,  ...,  1.2580e-01,\n",
       "            1.3209e-02, -9.9387e-02],\n",
       "          [-2.3857e-01, -2.2884e-01, -2.1910e-01,  ..., -1.8248e-01,\n",
       "           -1.1510e-01, -4.7735e-02],\n",
       "          [-9.2954e-02, -1.3412e-01, -1.7529e-01,  ..., -4.9076e-01,\n",
       "           -2.4342e-01,  3.9184e-03],\n",
       "          ...,\n",
       "          [-3.3510e-01, -2.5380e-01, -1.7249e-01,  ..., -2.4350e-01,\n",
       "           -3.1709e-01, -3.9067e-01],\n",
       "          [-1.3648e-01, -4.5333e-02,  4.5812e-02,  ..., -2.8623e-01,\n",
       "           -5.8303e-02,  1.6963e-01],\n",
       "          [ 6.2148e-02,  1.6313e-01,  2.6412e-01,  ..., -3.2897e-01,\n",
       "            2.0048e-01,  7.2993e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.1369e-02,  7.0422e-02,  1.2947e-01,  ...,  3.1386e-01,\n",
       "            1.3748e-01, -3.8906e-02],\n",
       "          [ 7.9130e-02,  4.6944e-02,  1.4758e-02,  ...,  3.6751e-01,\n",
       "            1.3419e-01, -9.9131e-02],\n",
       "          [ 1.4689e-01,  2.3466e-02, -9.9959e-02,  ...,  4.2116e-01,\n",
       "            1.3090e-01, -1.5936e-01],\n",
       "          ...,\n",
       "          [-1.5583e-01, -1.6795e-01, -1.8007e-01,  ...,  2.4459e-01,\n",
       "            3.0015e-01,  3.5572e-01],\n",
       "          [-8.7840e-02, -5.5999e-02, -2.4159e-02,  ...,  2.1754e-01,\n",
       "            2.7097e-01,  3.2441e-01],\n",
       "          [-1.9847e-02,  5.5954e-02,  1.3176e-01,  ...,  1.9049e-01,\n",
       "            2.4179e-01,  2.9309e-01]],\n",
       "\n",
       "         [[-1.8182e-01, -2.6174e-01, -3.4166e-01,  ..., -2.2447e-01,\n",
       "           -3.9556e-01, -5.6665e-01],\n",
       "          [-3.0619e-02, -1.6279e-01, -2.9495e-01,  ..., -2.1287e-01,\n",
       "           -3.7837e-01, -5.4388e-01],\n",
       "          [ 1.2058e-01, -6.3830e-02, -2.4824e-01,  ..., -2.0127e-01,\n",
       "           -3.6119e-01, -5.2111e-01],\n",
       "          ...,\n",
       "          [-2.3221e-01, -1.9037e-01, -1.4854e-01,  ...,  2.8103e-01,\n",
       "           -1.0205e-01, -4.8513e-01],\n",
       "          [-1.8343e-01, -2.4413e-01, -3.0484e-01,  ..., -1.4584e-01,\n",
       "           -3.3803e-01, -5.3023e-01],\n",
       "          [-1.3465e-01, -2.9789e-01, -4.6114e-01,  ..., -5.7271e-01,\n",
       "           -5.7402e-01, -5.7533e-01]],\n",
       "\n",
       "         [[ 1.9931e-01,  1.8295e-01,  1.6660e-01,  ..., -9.7440e-02,\n",
       "            7.1690e-02,  2.4082e-01],\n",
       "          [ 1.0692e-01,  1.0084e-01,  9.4763e-02,  ...,  1.1292e-01,\n",
       "            1.2291e-01,  1.3291e-01],\n",
       "          [ 1.4542e-02,  1.8732e-02,  2.2922e-02,  ...,  3.2328e-01,\n",
       "            1.7414e-01,  2.5000e-02],\n",
       "          ...,\n",
       "          [ 3.2634e-01,  5.0226e-02, -2.2588e-01,  ...,  3.6658e-01,\n",
       "            1.9743e-01,  2.8289e-02],\n",
       "          [ 2.9878e-02,  5.4086e-02,  7.8294e-02,  ...,  6.9031e-02,\n",
       "            1.4404e-01,  2.1904e-01],\n",
       "          [-2.6658e-01,  5.7946e-02,  3.8247e-01,  ..., -2.2852e-01,\n",
       "            9.0640e-02,  4.0980e-01]]],\n",
       "\n",
       "\n",
       "        [[[-3.5699e-01, -2.8302e-02,  3.0039e-01,  ...,  2.5872e-02,\n",
       "           -1.9527e-01, -4.1641e-01],\n",
       "          [-9.8170e-02, -1.6209e-01, -2.2601e-01,  ..., -3.5099e-01,\n",
       "           -3.5935e-01, -3.6770e-01],\n",
       "          [ 1.6065e-01, -2.9588e-01, -7.5241e-01,  ..., -7.2785e-01,\n",
       "           -5.2342e-01, -3.1900e-01],\n",
       "          ...,\n",
       "          [-5.8416e-01, -6.5785e-01, -7.3153e-01,  ..., -6.1468e-01,\n",
       "           -7.7266e-01, -9.3065e-01],\n",
       "          [-5.1490e-01, -5.5920e-01, -6.0349e-01,  ..., -5.1842e-01,\n",
       "           -5.7058e-01, -6.2273e-01],\n",
       "          [-4.4564e-01, -4.6055e-01, -4.7545e-01,  ..., -4.2217e-01,\n",
       "           -3.6849e-01, -3.1481e-01]],\n",
       "\n",
       "         [[-6.2968e-01, -1.0873e-01,  4.1222e-01,  ...,  3.8030e-02,\n",
       "            8.9844e-02,  1.4166e-01],\n",
       "          [-4.5415e-01, -1.6125e-01,  1.3164e-01,  ...,  6.7650e-02,\n",
       "            1.6912e-01,  2.7058e-01],\n",
       "          [-2.7862e-01, -2.1378e-01, -1.4894e-01,  ...,  9.7271e-02,\n",
       "            2.4839e-01,  3.9951e-01],\n",
       "          ...,\n",
       "          [-4.1291e-01, -1.0747e-01,  1.9797e-01,  ...,  5.7899e-01,\n",
       "            3.1403e-01,  4.9071e-02],\n",
       "          [-1.6708e-02,  6.9710e-02,  1.5613e-01,  ...,  5.0077e-02,\n",
       "            1.1624e-01,  1.8239e-01],\n",
       "          [ 3.7949e-01,  2.4689e-01,  1.1428e-01,  ..., -4.7883e-01,\n",
       "           -8.1557e-02,  3.1572e-01]],\n",
       "\n",
       "         [[-4.5488e-01, -1.4024e-01,  1.7439e-01,  ..., -4.7776e-01,\n",
       "           -3.0608e-01, -1.3440e-01],\n",
       "          [-4.7097e-01, -9.2594e-02,  2.8579e-01,  ..., -7.6429e-02,\n",
       "           -4.2626e-02, -8.8229e-03],\n",
       "          [-4.8706e-01, -4.4944e-02,  3.9718e-01,  ...,  3.2490e-01,\n",
       "            2.2083e-01,  1.1676e-01],\n",
       "          ...,\n",
       "          [ 8.1540e-02,  1.0636e-01,  1.3118e-01,  ..., -3.4837e-01,\n",
       "           -2.0822e-01, -6.8076e-02],\n",
       "          [-3.2423e-02,  5.9568e-02,  1.5156e-01,  ...,  4.2291e-02,\n",
       "            2.5934e-02,  9.5766e-03],\n",
       "          [-1.4639e-01,  1.2775e-02,  1.7194e-01,  ...,  4.3295e-01,\n",
       "            2.6009e-01,  8.7229e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 3.5395e-01,  2.6077e-01,  1.6758e-01,  ...,  1.4986e-01,\n",
       "            5.2710e-02, -4.4440e-02],\n",
       "          [ 3.6842e-01,  1.9402e-01,  1.9625e-02,  ..., -7.4590e-02,\n",
       "           -4.1605e-02, -8.6195e-03],\n",
       "          [ 3.8288e-01,  1.2728e-01, -1.2833e-01,  ..., -2.9904e-01,\n",
       "           -1.3592e-01,  2.7201e-02],\n",
       "          ...,\n",
       "          [ 2.5173e-01,  3.4723e-01,  4.4272e-01,  ...,  2.9326e-01,\n",
       "            1.6244e-01,  3.1618e-02],\n",
       "          [ 2.5356e-01,  3.0537e-01,  3.5719e-01,  ...,  1.3112e-01,\n",
       "            9.8235e-02,  6.5352e-02],\n",
       "          [ 2.5539e-01,  2.6352e-01,  2.7166e-01,  ..., -3.1018e-02,\n",
       "            3.4034e-02,  9.9086e-02]],\n",
       "\n",
       "         [[-3.1079e-01, -2.9823e-01, -2.8567e-01,  ..., -6.8933e-01,\n",
       "           -3.3407e-01,  2.1189e-02],\n",
       "          [-1.8629e-01, -2.0198e-01, -2.1767e-01,  ..., -5.5347e-01,\n",
       "           -3.6443e-01, -1.7540e-01],\n",
       "          [-6.1792e-02, -1.0573e-01, -1.4966e-01,  ..., -4.1761e-01,\n",
       "           -3.9480e-01, -3.7199e-01],\n",
       "          ...,\n",
       "          [ 5.5490e-02,  9.8384e-02,  1.4128e-01,  ..., -1.1626e-01,\n",
       "           -2.5801e-01, -3.9975e-01],\n",
       "          [-9.1331e-02,  4.1530e-02,  1.7439e-01,  ..., -6.0622e-02,\n",
       "           -1.4693e-01, -2.3323e-01],\n",
       "          [-2.3815e-01, -1.5323e-02,  2.0751e-01,  ..., -4.9798e-03,\n",
       "           -3.5849e-02, -6.6717e-02]],\n",
       "\n",
       "         [[ 1.6274e-01,  2.1887e-01,  2.7500e-01,  ...,  2.2394e-01,\n",
       "            1.0468e-01, -1.4577e-02],\n",
       "          [ 8.0438e-02,  1.7885e-01,  2.7726e-01,  ...,  7.8047e-02,\n",
       "            1.9674e-01,  3.1544e-01],\n",
       "          [-1.8629e-03,  1.3882e-01,  2.7951e-01,  ..., -6.7846e-02,\n",
       "            2.8881e-01,  6.4546e-01],\n",
       "          ...,\n",
       "          [ 3.9118e-01,  4.2268e-01,  4.5417e-01,  ...,  1.2131e-01,\n",
       "            4.6065e-01,  7.9998e-01],\n",
       "          [ 4.8975e-01,  3.9648e-01,  3.0321e-01,  ...,  2.5255e-01,\n",
       "            3.7622e-01,  4.9988e-01],\n",
       "          [ 5.8833e-01,  3.7029e-01,  1.5225e-01,  ...,  3.8379e-01,\n",
       "            2.9179e-01,  1.9978e-01]]],\n",
       "\n",
       "\n",
       "        [[[-7.4813e-02, -9.7295e-02, -1.1978e-01,  ..., -3.6441e-01,\n",
       "           -3.3369e-01, -3.0297e-01],\n",
       "          [-2.6834e-01, -3.3930e-01, -4.1026e-01,  ..., -4.5392e-01,\n",
       "           -4.3614e-01, -4.1836e-01],\n",
       "          [-4.6187e-01, -5.8131e-01, -7.0074e-01,  ..., -5.4343e-01,\n",
       "           -5.3859e-01, -5.3374e-01],\n",
       "          ...,\n",
       "          [-3.3141e-01, -6.2393e-01, -9.1646e-01,  ..., -6.3066e-01,\n",
       "           -7.0705e-01, -7.8345e-01],\n",
       "          [-1.5584e-01, -3.1768e-01, -4.7951e-01,  ..., -6.8185e-01,\n",
       "           -8.0302e-01, -9.2420e-01],\n",
       "          [ 1.9726e-02, -1.1422e-02, -4.2570e-02,  ..., -7.3304e-01,\n",
       "           -8.9899e-01, -1.0649e+00]],\n",
       "\n",
       "         [[-3.2225e-01, -3.6084e-01, -3.9942e-01,  ...,  1.0155e-01,\n",
       "           -6.1644e-02, -2.2484e-01],\n",
       "          [-7.8375e-02, -6.1404e-02, -4.4434e-02,  ...,  1.0821e-01,\n",
       "           -8.8721e-02, -2.8565e-01],\n",
       "          [ 1.6550e-01,  2.3803e-01,  3.1055e-01,  ...,  1.1487e-01,\n",
       "           -1.1580e-01, -3.4646e-01],\n",
       "          ...,\n",
       "          [-2.3350e-02,  6.8456e-02,  1.6026e-01,  ...,  1.6429e-01,\n",
       "            1.1909e-01,  7.3892e-02],\n",
       "          [-2.2566e-01, -5.8175e-02,  1.0931e-01,  ...,  1.2123e-01,\n",
       "            4.3109e-02, -3.5013e-02],\n",
       "          [-4.2798e-01, -1.8481e-01,  5.8364e-02,  ...,  7.8169e-02,\n",
       "           -3.2874e-02, -1.4392e-01]],\n",
       "\n",
       "         [[-2.5917e-01, -1.5092e-01, -4.2662e-02,  ..., -1.0061e-01,\n",
       "           -1.8874e-01, -2.7687e-01],\n",
       "          [-1.4817e-01, -1.2891e-01, -1.0965e-01,  ...,  1.3250e-01,\n",
       "           -5.9526e-02, -2.5155e-01],\n",
       "          [-3.7163e-02, -1.0690e-01, -1.7664e-01,  ...,  3.6561e-01,\n",
       "            6.9687e-02, -2.2623e-01],\n",
       "          ...,\n",
       "          [ 8.0459e-02, -1.1864e-01, -3.1775e-01,  ...,  1.3967e-01,\n",
       "            7.0552e-02,  1.4386e-03],\n",
       "          [-9.0228e-02, -2.0847e-01, -3.2671e-01,  ..., -1.1010e-02,\n",
       "           -5.9027e-02, -1.0704e-01],\n",
       "          [-2.6092e-01, -2.9830e-01, -3.3568e-01,  ..., -1.6169e-01,\n",
       "           -1.8861e-01, -2.1553e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 2.4614e-01,  4.9383e-02, -1.4738e-01,  ...,  2.7228e-01,\n",
       "            8.4073e-02, -1.0413e-01],\n",
       "          [ 1.7428e-01, -6.6262e-03, -1.8753e-01,  ...,  4.5042e-01,\n",
       "            2.4148e-01,  3.2535e-02],\n",
       "          [ 1.0242e-01, -6.2635e-02, -2.2769e-01,  ...,  6.2857e-01,\n",
       "            3.9888e-01,  1.6920e-01],\n",
       "          ...,\n",
       "          [ 4.7454e-01,  4.9860e-01,  5.2266e-01,  ..., -7.5048e-02,\n",
       "            1.0252e-02,  9.5551e-02],\n",
       "          [ 2.3174e-01,  4.6204e-01,  6.9233e-01,  ..., -7.0499e-03,\n",
       "            6.1993e-02,  1.3104e-01],\n",
       "          [-1.1052e-02,  4.2548e-01,  8.6201e-01,  ...,  6.0948e-02,\n",
       "            1.1373e-01,  1.6652e-01]],\n",
       "\n",
       "         [[ 1.7921e-01, -2.6839e-02, -2.3289e-01,  ..., -1.5307e-01,\n",
       "           -1.8698e-01, -2.2088e-01],\n",
       "          [ 1.4648e-01, -1.1767e-02, -1.7001e-01,  ...,  1.1834e-01,\n",
       "            1.3406e-02, -9.1531e-02],\n",
       "          [ 1.1375e-01,  3.3051e-03, -1.0714e-01,  ...,  3.8976e-01,\n",
       "            2.1379e-01,  3.7820e-02],\n",
       "          ...,\n",
       "          [-1.8277e-01, -6.7030e-03,  1.6937e-01,  ..., -2.1935e-01,\n",
       "           -5.8116e-02,  1.0312e-01],\n",
       "          [-1.4265e-01, -1.3402e-01, -1.2539e-01,  ..., -2.1447e-01,\n",
       "           -1.9584e-01, -1.7722e-01],\n",
       "          [-1.0254e-01, -2.6134e-01, -4.2015e-01,  ..., -2.0958e-01,\n",
       "           -3.3357e-01, -4.5756e-01]],\n",
       "\n",
       "         [[ 8.7200e-02,  3.9193e-01,  6.9666e-01,  ...,  6.2607e-01,\n",
       "            4.8702e-01,  3.4798e-01],\n",
       "          [ 1.5320e-01,  3.3960e-01,  5.2601e-01,  ...,  4.8137e-01,\n",
       "            4.1010e-01,  3.3884e-01],\n",
       "          [ 2.1919e-01,  2.8728e-01,  3.5536e-01,  ...,  3.3666e-01,\n",
       "            3.3319e-01,  3.2971e-01],\n",
       "          ...,\n",
       "          [-8.6512e-02,  8.9728e-02,  2.6597e-01,  ...,  4.4082e-01,\n",
       "            5.0500e-01,  5.6919e-01],\n",
       "          [-4.7817e-02,  8.8969e-02,  2.2576e-01,  ...,  3.5937e-01,\n",
       "            3.4763e-01,  3.3588e-01],\n",
       "          [-9.1229e-03,  8.8210e-02,  1.8554e-01,  ...,  2.7793e-01,\n",
       "            1.9025e-01,  1.0257e-01]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[-2.4693e-01, -5.6941e-03,  2.3554e-01,  ...,  1.0605e-02,\n",
       "           -1.8168e-01, -3.7397e-01],\n",
       "          [-2.6856e-01, -6.3074e-02,  1.4241e-01,  ..., -3.6195e-01,\n",
       "           -3.4682e-01, -3.3169e-01],\n",
       "          [-2.9018e-01, -1.2045e-01,  4.9276e-02,  ..., -7.3451e-01,\n",
       "           -5.1196e-01, -2.8941e-01],\n",
       "          ...,\n",
       "          [ 1.7341e-01, -2.2842e-01, -6.3026e-01,  ..., -3.9601e-01,\n",
       "           -4.4076e-01, -4.8552e-01],\n",
       "          [ 3.6899e-02, -2.9867e-01, -6.3424e-01,  ..., -4.2349e-01,\n",
       "           -4.6980e-01, -5.1610e-01],\n",
       "          [-9.9617e-02, -3.6891e-01, -6.3821e-01,  ..., -4.5097e-01,\n",
       "           -4.9883e-01, -5.4668e-01]],\n",
       "\n",
       "         [[-2.9080e-01, -7.2698e-02,  1.4541e-01,  ...,  9.1561e-02,\n",
       "            5.1784e-02,  1.2008e-02],\n",
       "          [-5.3093e-02,  4.9290e-02,  1.5167e-01,  ...,  2.3107e-01,\n",
       "            1.5327e-01,  7.5468e-02],\n",
       "          [ 1.8462e-01,  1.7128e-01,  1.5794e-01,  ...,  3.7058e-01,\n",
       "            2.5475e-01,  1.3893e-01],\n",
       "          ...,\n",
       "          [ 4.8331e-01,  2.7914e-01,  7.4973e-02,  ..., -2.1900e-01,\n",
       "           -7.0135e-02,  7.8732e-02],\n",
       "          [ 2.6088e-01,  1.7659e-01,  9.2291e-02,  ..., -6.0410e-02,\n",
       "            6.0498e-03,  7.2510e-02],\n",
       "          [ 3.8455e-02,  7.4032e-02,  1.0961e-01,  ...,  9.8182e-02,\n",
       "            8.2235e-02,  6.6287e-02]],\n",
       "\n",
       "         [[-1.6694e-02, -8.0069e-02, -1.4345e-01,  ..., -1.9119e-01,\n",
       "           -6.5547e-02,  6.0100e-02],\n",
       "          [-2.0200e-01, -1.5684e-01, -1.1168e-01,  ..., -6.4105e-02,\n",
       "            2.8694e-02,  1.2149e-01],\n",
       "          [-3.8731e-01, -2.3361e-01, -7.9917e-02,  ...,  6.2985e-02,\n",
       "            1.2293e-01,  1.8288e-01],\n",
       "          ...,\n",
       "          [-9.7843e-02,  1.5566e-01,  4.0917e-01,  ..., -1.4888e-01,\n",
       "           -2.6981e-01, -3.9073e-01],\n",
       "          [ 3.4257e-02,  1.4590e-01,  2.5754e-01,  ...,  1.3344e-01,\n",
       "           -3.4523e-02, -2.0249e-01],\n",
       "          [ 1.6636e-01,  1.3613e-01,  1.0590e-01,  ...,  4.1577e-01,\n",
       "            2.0076e-01, -1.4241e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 3.2545e-01,  1.8711e-01,  4.8776e-02,  ...,  1.4990e-01,\n",
       "            2.0427e-01,  2.5864e-01],\n",
       "          [ 1.7925e-01,  1.2204e-01,  6.4822e-02,  ..., -1.6114e-01,\n",
       "           -6.6390e-02,  2.8355e-02],\n",
       "          [ 3.3052e-02,  5.6960e-02,  8.0869e-02,  ..., -4.7217e-01,\n",
       "           -3.3705e-01, -2.0193e-01],\n",
       "          ...,\n",
       "          [ 4.6905e-01,  3.0480e-01,  1.4055e-01,  ...,  5.8578e-01,\n",
       "            1.8125e-01, -2.2327e-01],\n",
       "          [ 2.7266e-01,  2.8299e-01,  2.9333e-01,  ...,  5.9774e-01,\n",
       "            3.2594e-01,  5.4138e-02],\n",
       "          [ 7.6270e-02,  2.6119e-01,  4.4610e-01,  ...,  6.0971e-01,\n",
       "            4.7063e-01,  3.3155e-01]],\n",
       "\n",
       "         [[-2.5490e-02, -3.9794e-02, -5.4098e-02,  ..., -4.9581e-02,\n",
       "           -1.6652e-01, -2.8345e-01],\n",
       "          [-1.4595e-01, -2.2320e-01, -3.0045e-01,  ..., -2.4051e-01,\n",
       "           -2.1677e-01, -1.9302e-01],\n",
       "          [-2.6640e-01, -4.0661e-01, -5.4681e-01,  ..., -4.3145e-01,\n",
       "           -2.6702e-01, -1.0260e-01],\n",
       "          ...,\n",
       "          [ 1.4370e-02, -1.9236e-01, -3.9909e-01,  ..., -6.4603e-01,\n",
       "           -3.8608e-01, -1.2614e-01],\n",
       "          [-9.1423e-02, -2.1617e-01, -3.4092e-01,  ..., -5.5331e-01,\n",
       "           -3.1752e-01, -8.1724e-02],\n",
       "          [-1.9722e-01, -2.3998e-01, -2.8275e-01,  ..., -4.6059e-01,\n",
       "           -2.4895e-01, -3.7308e-02]],\n",
       "\n",
       "         [[-6.6981e-02,  1.9495e-01,  4.5688e-01,  ...,  5.6038e-01,\n",
       "            4.5786e-01,  3.5534e-01],\n",
       "          [ 9.1091e-02,  1.5547e-01,  2.1986e-01,  ...,  3.7320e-01,\n",
       "            3.5810e-01,  3.4300e-01],\n",
       "          [ 2.4916e-01,  1.1600e-01, -1.7167e-02,  ...,  1.8601e-01,\n",
       "            2.5834e-01,  3.3067e-01],\n",
       "          ...,\n",
       "          [ 2.4611e-01,  7.0484e-02, -1.0514e-01,  ...,  3.5423e-01,\n",
       "            2.7277e-01,  1.9131e-01],\n",
       "          [ 1.9631e-01,  1.8279e-01,  1.6927e-01,  ...,  2.8907e-01,\n",
       "            2.5337e-01,  2.1766e-01],\n",
       "          [ 1.4651e-01,  2.9510e-01,  4.4369e-01,  ...,  2.2391e-01,\n",
       "            2.3396e-01,  2.4402e-01]]],\n",
       "\n",
       "\n",
       "        [[[-6.0283e-01, -1.8897e-01,  2.2490e-01,  ...,  1.7418e-02,\n",
       "           -1.3058e-01, -2.7857e-01],\n",
       "          [-2.6380e-01, -1.4561e-01, -2.7419e-02,  ..., -4.1770e-01,\n",
       "           -3.3130e-01, -2.4491e-01],\n",
       "          [ 7.5238e-02, -1.0225e-01, -2.7974e-01,  ..., -8.5281e-01,\n",
       "           -5.3203e-01, -2.1125e-01],\n",
       "          ...,\n",
       "          [-4.4963e-01, -4.7691e-01, -5.0419e-01,  ..., -1.0017e+00,\n",
       "           -7.1584e-01, -4.2995e-01],\n",
       "          [-3.3740e-01, -3.8905e-01, -4.4070e-01,  ..., -8.8737e-01,\n",
       "           -7.0811e-01, -5.2885e-01],\n",
       "          [-2.2517e-01, -3.0120e-01, -3.7722e-01,  ..., -7.7301e-01,\n",
       "           -7.0038e-01, -6.2775e-01]],\n",
       "\n",
       "         [[ 1.4451e-02, -2.0719e-01, -4.2883e-01,  ...,  3.3884e-01,\n",
       "            2.1536e-01,  9.1880e-02],\n",
       "          [-4.0720e-02,  5.8551e-02,  1.5782e-01,  ..., -2.4018e-01,\n",
       "           -1.4407e-01, -4.7966e-02],\n",
       "          [-9.5892e-02,  3.2429e-01,  7.4447e-01,  ..., -8.1920e-01,\n",
       "           -5.0350e-01, -1.8781e-01],\n",
       "          ...,\n",
       "          [ 3.9525e-01,  4.5382e-02, -3.0448e-01,  ...,  5.9655e-01,\n",
       "            4.7179e-01,  3.4702e-01],\n",
       "          [ 3.1751e-01,  3.0101e-02, -2.5731e-01,  ...,  3.2920e-02,\n",
       "            1.2551e-01,  2.1810e-01],\n",
       "          [ 2.3977e-01,  1.4820e-02, -2.1013e-01,  ..., -5.3071e-01,\n",
       "           -2.2077e-01,  8.9172e-02]],\n",
       "\n",
       "         [[-5.0286e-02, -1.5094e-02,  2.0098e-02,  ..., -1.1433e-01,\n",
       "           -9.4508e-02, -7.4686e-02],\n",
       "          [-1.3179e-01, -1.9279e-02,  9.3237e-02,  ..., -2.1926e-03,\n",
       "           -2.4370e-02, -4.6547e-02],\n",
       "          [-2.1330e-01, -2.3463e-02,  1.6638e-01,  ...,  1.0994e-01,\n",
       "            4.5769e-02, -1.8407e-02],\n",
       "          ...,\n",
       "          [-1.4977e-01, -4.6541e-01, -7.8104e-01,  ...,  8.3224e-02,\n",
       "           -9.7283e-02, -2.7779e-01],\n",
       "          [-1.0787e-01, -2.2075e-01, -3.3362e-01,  ...,  4.5738e-02,\n",
       "           -1.2124e-01, -2.8821e-01],\n",
       "          [-6.5966e-02,  2.3912e-02,  1.1379e-01,  ...,  8.2521e-03,\n",
       "           -1.4519e-01, -2.9863e-01]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 1.7049e-01,  2.9320e-01,  4.1592e-01,  ...,  2.6040e-01,\n",
       "            9.1777e-02, -7.6847e-02],\n",
       "          [ 6.7451e-02,  3.0314e-01,  5.3882e-01,  ...,  3.5189e-01,\n",
       "            2.1156e-01,  7.1234e-02],\n",
       "          [-3.5586e-02,  3.1307e-01,  6.6173e-01,  ...,  4.4337e-01,\n",
       "            3.3134e-01,  2.1932e-01],\n",
       "          ...,\n",
       "          [ 2.0347e-01,  2.2302e-01,  2.4256e-01,  ...,  5.0931e-01,\n",
       "            3.0729e-01,  1.0528e-01],\n",
       "          [ 1.1689e-01,  3.6979e-02, -4.2933e-02,  ...,  2.0232e-01,\n",
       "            1.1805e-01,  3.3773e-02],\n",
       "          [ 3.0312e-02, -1.4906e-01, -3.2843e-01,  ..., -1.0467e-01,\n",
       "           -7.1204e-02, -3.7734e-02]],\n",
       "\n",
       "         [[ 2.0485e-01,  8.7383e-02, -3.0083e-02,  ..., -6.1449e-02,\n",
       "           -2.4585e-01, -4.3025e-01],\n",
       "          [-2.0006e-02, -1.0420e-02, -8.3472e-04,  ..., -1.2717e-01,\n",
       "           -2.0016e-01, -2.7316e-01],\n",
       "          [-2.4486e-01, -1.0822e-01,  2.8413e-02,  ..., -1.9288e-01,\n",
       "           -1.5448e-01, -1.1608e-01],\n",
       "          ...,\n",
       "          [-2.8130e-01, -2.8273e-01, -2.8416e-01,  ..., -1.7201e-01,\n",
       "           -2.5751e-01, -3.4301e-01],\n",
       "          [-4.0455e-02, -2.4708e-01, -4.5370e-01,  ..., -1.9092e-01,\n",
       "           -1.9358e-01, -1.9623e-01],\n",
       "          [ 2.0039e-01, -2.1142e-01, -6.2324e-01,  ..., -2.0983e-01,\n",
       "           -1.2965e-01, -4.9461e-02]],\n",
       "\n",
       "         [[ 1.8890e-01,  1.3538e-01,  8.1853e-02,  ...,  5.8056e-01,\n",
       "            4.5550e-01,  3.3044e-01],\n",
       "          [ 4.4301e-01,  3.3352e-01,  2.2402e-01,  ...,  5.7321e-01,\n",
       "            4.6864e-01,  3.6407e-01],\n",
       "          [ 6.9712e-01,  5.3166e-01,  3.6620e-01,  ...,  5.6586e-01,\n",
       "            4.8178e-01,  3.9771e-01],\n",
       "          ...,\n",
       "          [ 1.8381e-01,  2.6952e-01,  3.5524e-01,  ...,  1.1169e+00,\n",
       "            7.7589e-01,  4.3486e-01],\n",
       "          [ 1.5879e-01,  1.9394e-01,  2.2908e-01,  ...,  7.8934e-01,\n",
       "            4.4977e-01,  1.1019e-01],\n",
       "          [ 1.3378e-01,  1.1835e-01,  1.0292e-01,  ...,  4.6176e-01,\n",
       "            1.2364e-01, -2.1447e-01]]],\n",
       "\n",
       "\n",
       "        [[[-2.2500e-01, -1.6370e-01, -1.0241e-01,  ..., -3.2517e-01,\n",
       "           -3.7134e-01, -4.1752e-01],\n",
       "          [-2.3784e-01, -2.1494e-01, -1.9203e-01,  ..., -2.4862e-01,\n",
       "           -4.5416e-01, -6.5971e-01],\n",
       "          [-2.5068e-01, -2.6617e-01, -2.8166e-01,  ..., -1.7206e-01,\n",
       "           -5.3698e-01, -9.0190e-01],\n",
       "          ...,\n",
       "          [-3.0823e-01,  6.2704e-02,  4.3363e-01,  ..., -7.7269e-01,\n",
       "           -6.4798e-01, -5.2328e-01],\n",
       "          [-2.7689e-01, -1.6555e-01, -5.4212e-02,  ..., -7.1595e-01,\n",
       "           -5.5544e-01, -3.9492e-01],\n",
       "          [-2.4555e-01, -3.9381e-01, -5.4206e-01,  ..., -6.5922e-01,\n",
       "           -4.6289e-01, -2.6657e-01]],\n",
       "\n",
       "         [[-4.0508e-01, -2.6429e-01, -1.2351e-01,  ..., -4.3571e-01,\n",
       "           -2.7663e-01, -1.1755e-01],\n",
       "          [-3.3959e-02,  1.4469e-02,  6.2898e-02,  ..., -3.7544e-01,\n",
       "           -1.6527e-01,  4.4897e-02],\n",
       "          [ 3.3716e-01,  2.9323e-01,  2.4930e-01,  ..., -3.1518e-01,\n",
       "           -5.3922e-02,  2.0734e-01],\n",
       "          ...,\n",
       "          [-2.3760e-01, -3.3589e-01, -4.3418e-01,  ..., -5.1005e-01,\n",
       "            4.6171e-02,  6.0239e-01],\n",
       "          [-1.3834e-01, -1.6519e-01, -1.9205e-01,  ..., -3.3933e-01,\n",
       "            7.8610e-03,  3.5505e-01],\n",
       "          [-3.9079e-02,  5.5017e-03,  5.0082e-02,  ..., -1.6860e-01,\n",
       "           -3.0449e-02,  1.0770e-01]],\n",
       "\n",
       "         [[-6.9053e-01, -3.6026e-01, -2.9980e-02,  ..., -2.6262e-01,\n",
       "           -1.0808e-03,  2.6046e-01],\n",
       "          [-3.1457e-01, -9.2253e-02,  1.3006e-01,  ...,  3.2801e-02,\n",
       "            1.3546e-01,  2.3812e-01],\n",
       "          [ 6.1394e-02,  1.7575e-01,  2.9010e-01,  ...,  3.2822e-01,\n",
       "            2.7200e-01,  2.1578e-01],\n",
       "          ...,\n",
       "          [-3.1857e-01, -2.7776e-01, -2.3694e-01,  ...,  1.7214e-01,\n",
       "            2.0194e-01,  2.3174e-01],\n",
       "          [-1.5605e-01, -2.1155e-01, -2.6706e-01,  ...,  2.1604e-01,\n",
       "            1.8673e-01,  1.5742e-01],\n",
       "          [ 6.4724e-03, -1.4535e-01, -2.9717e-01,  ...,  2.5994e-01,\n",
       "            1.7152e-01,  8.3102e-02]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[ 3.7920e-01,  4.1317e-01,  4.4714e-01,  ...,  3.9301e-01,\n",
       "            2.2217e-01,  5.1326e-02],\n",
       "          [ 1.7302e-01,  3.4548e-01,  5.1794e-01,  ...,  2.9894e-01,\n",
       "            4.2754e-02, -2.1344e-01],\n",
       "          [-3.3173e-02,  2.7778e-01,  5.8874e-01,  ...,  2.0488e-01,\n",
       "           -1.3666e-01, -4.7820e-01],\n",
       "          ...,\n",
       "          [ 5.0232e-01,  4.4684e-01,  3.9136e-01,  ..., -2.8081e-01,\n",
       "           -2.4205e-01, -2.0330e-01],\n",
       "          [ 3.3203e-01,  3.9643e-01,  4.6083e-01,  ..., -1.0834e-01,\n",
       "           -1.0814e-02,  8.6716e-02],\n",
       "          [ 1.6174e-01,  3.4603e-01,  5.3031e-01,  ...,  6.4120e-02,\n",
       "            2.2043e-01,  3.7673e-01]],\n",
       "\n",
       "         [[ 7.4618e-03, -3.4866e-01, -7.0478e-01,  ..., -3.0467e-01,\n",
       "           -4.1120e-01, -5.1772e-01],\n",
       "          [ 4.0014e-02, -4.2238e-02, -1.2449e-01,  ..., -3.6983e-01,\n",
       "           -4.1620e-01, -4.6257e-01],\n",
       "          [ 7.2566e-02,  2.6418e-01,  4.5580e-01,  ..., -4.3499e-01,\n",
       "           -4.2120e-01, -4.0741e-01],\n",
       "          ...,\n",
       "          [ 4.2901e-01, -7.5091e-02, -5.7919e-01,  ..., -9.5809e-01,\n",
       "           -6.5989e-01, -3.6168e-01],\n",
       "          [ 1.7793e-01,  3.7989e-02, -1.0195e-01,  ..., -6.8460e-01,\n",
       "           -4.2374e-01, -1.6288e-01],\n",
       "          [-7.3156e-02,  1.5107e-01,  3.7529e-01,  ..., -4.1110e-01,\n",
       "           -1.8759e-01,  3.5923e-02]],\n",
       "\n",
       "         [[ 4.8198e-02,  1.4334e-01,  2.3849e-01,  ...,  3.2487e-01,\n",
       "            5.2608e-01,  7.2728e-01],\n",
       "          [ 9.7116e-02,  7.1958e-02,  4.6800e-02,  ...,  3.2032e-01,\n",
       "            3.6109e-01,  4.0186e-01],\n",
       "          [ 1.4603e-01,  5.7259e-04, -1.4489e-01,  ...,  3.1576e-01,\n",
       "            1.9610e-01,  7.6439e-02],\n",
       "          ...,\n",
       "          [ 3.2306e-01,  4.7601e-01,  6.2897e-01,  ...,  6.3080e-01,\n",
       "            3.8380e-01,  1.3680e-01],\n",
       "          [ 3.0341e-01,  4.9942e-01,  6.9542e-01,  ...,  6.1813e-01,\n",
       "            5.1144e-01,  4.0474e-01],\n",
       "          [ 2.8377e-01,  5.2282e-01,  7.6186e-01,  ...,  6.0547e-01,\n",
       "            6.3908e-01,  6.7269e-01]]]], grad_fn=<UpsampleBilinear2DBackward1>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# import F from torch\n",
    "from torch.nn import functional as F\n",
    "class UpSample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        ''' \n",
    "        low-resolution input to the MSFU-Block came from the bottom- or another MSFU-Block\n",
    "        high-resolution came from MMFF-Block\n",
    "        '''\n",
    "        super(UpSample, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.upsample(x)\n",
    "        return x\n",
    "x_mmff_5 =  torch.rand([12, 768, 116, 116])\n",
    "x_mmff_4 =  torch.rand([12, 384, 240, 240])\n",
    "x_mmff_3 =  torch.rand([12, 192, 244, 244])\n",
    "x_mmff_2 =  torch.rand([12, 96, 248, 248])\n",
    "x_mmff_1 =  torch.rand([12, 3, 252, 252])\n",
    "\n",
    "up_sample = UpSample(128*3, 64*3)\n",
    "up_sample(x_mmff_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_1 shape =  torch.Size([3, 256, 256])\n",
      "x_2 shape =  torch.Size([3, 256, 256])\n",
      "x shape =  torch.Size([2, 3, 256, 256])\n",
      "tensor(1.3481, grad_fn=<MeanBackward0>)\n",
      "x_1 shape =  torch.Size([3, 256, 256])\n",
      "x_2 shape =  torch.Size([3, 256, 256])\n",
      "x shape =  torch.Size([2, 3, 256, 256])\n",
      "tensor(1.3481, grad_fn=<MeanBackward0>)\n",
      "x_1 shape =  torch.Size([3, 256, 256])\n",
      "x_2 shape =  torch.Size([3, 256, 256])\n",
      "x shape =  torch.Size([2, 3, 256, 256])\n",
      "tensor(1.3481, grad_fn=<MeanBackward0>)\n",
      "x_1 shape =  torch.Size([3, 256, 256])\n",
      "x_2 shape =  torch.Size([3, 256, 256])\n",
      "x shape =  torch.Size([2, 3, 256, 256])\n",
      "tensor(1.3481, grad_fn=<MeanBackward0>)\n",
      "x_1 shape =  torch.Size([3, 256, 256])\n",
      "x_2 shape =  torch.Size([3, 256, 256])\n",
      "x shape =  torch.Size([2, 3, 256, 256])\n",
      "tensor(1.3481, grad_fn=<MeanBackward0>)\n",
      "x_1 shape =  torch.Size([3, 256, 256])\n",
      "x_2 shape =  torch.Size([3, 256, 256])\n",
      "x shape =  torch.Size([2, 3, 256, 256])\n",
      "tensor(1.3481, grad_fn=<MeanBackward0>)\n",
      "x_1 shape =  torch.Size([3, 256, 256])\n",
      "x_2 shape =  torch.Size([3, 256, 256])\n",
      "x shape =  torch.Size([2, 3, 256, 256])\n",
      "tensor(1.3481, grad_fn=<MeanBackward0>)\n",
      "x_1 shape =  torch.Size([3, 256, 256])\n",
      "x_2 shape =  torch.Size([3, 256, 256])\n",
      "x shape =  torch.Size([2, 3, 256, 256])\n",
      "tensor(1.3481, grad_fn=<MeanBackward0>)\n",
      "x_1 shape =  torch.Size([3, 256, 256])\n",
      "x_2 shape =  torch.Size([3, 256, 256])\n",
      "x shape =  torch.Size([2, 3, 256, 256])\n",
      "tensor(1.3481, grad_fn=<MeanBackward0>)\n",
      "x_1 shape =  torch.Size([3, 256, 256])\n",
      "x_2 shape =  torch.Size([3, 256, 256])\n",
      "x shape =  torch.Size([2, 3, 256, 256])\n",
      "tensor(1.3481, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn \n",
    "import torch\n",
    "class model_ok(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        # stack the input\n",
    "        print('x_1 shape = ', x1.shape)\n",
    "        print('x_2 shape = ', x2.shape)\n",
    "        \n",
    "        x = torch.stack([x1, x2], dim = 0) \n",
    "        print('x shape = ', x.shape)\n",
    "        x = self.conv1(x)\n",
    "        return x\n",
    "    \n",
    "x_1 = torch.randn( 3, 256, 256)\n",
    "x_2 = torch.randn( 3, 256, 256)\n",
    "\n",
    "model = model_ok()\n",
    "y_truth = torch.randn(2, 64, 256, 256)\n",
    "\n",
    "for i in range(10):\n",
    "    y_pred = model(x_1, x_2)\n",
    "    loss = torch.mean((y_truth - y_pred)**2)\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tiramisu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1bc4bb56b97818bf060df32d74a499d3c676370c9fb8148800d4da71b8d7e8e7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
